# Attention-based NMT model built by Chainer  

This is a LSTM NMT model of Japanese-English translation using Chianer 1.24. The main idea is baesd on the attention model proposed in the paper: [Neural machine translation by jointly learning to align and translate](https://arxiv.org/pdf/1409.0473.pdf). 

It adopted the 'global attention with dot product' introduced in the paper called [Effective Approaches to Attention-based Neural Machine Translation
](http://www.aclweb.org/anthology/D15-1166).  
It adopted dropout introduced in the paper: [RECURRENT NEURAL NETWORK
REGULARIZATION](https://arxiv.org/pdf/1409.2329.pdf).  

For mroe information about Chainer, please refer to the [chainer documentation](https://docs.chainer.org/en/latest/).
